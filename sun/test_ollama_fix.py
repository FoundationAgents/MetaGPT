#!/usr/bin/env python3
"""
æµ‹è¯• Ollama API ä¿®å¤æ•ˆæœ
éªŒè¯ JSON è§£æå’Œé”™è¯¯å¤„ç†æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import asyncio
import json
from metagpt.configs.llm_config import LLMConfig, LLMType
from metagpt.provider.ollama_api import OllamaLLM

async def test_ollama_api():
    """æµ‹è¯• Ollama API ä¿®å¤æ•ˆæœ"""
    print("=== æµ‹è¯• Ollama API ä¿®å¤æ•ˆæœ ===")
    
    try:
        # åˆ›å»º Ollama é…ç½®
        config = LLMConfig(
            api_type=LLMType.OLLAMA,
            model="qwen2.5:7b",
            base_url="http://127.0.0.1:11434/api/",  # ä¿®æ”¹ä¸ºå¸¦ /api/ è·¯å¾„
            api_key="dummy-key",  # Ollama ä¸éœ€è¦çœŸå®çš„ API keyï¼Œä½†éœ€è¦éç©ºå€¼
            timeout=600,
            temperature=0.1,
            stream=False
        )
        
        # åˆ›å»º Ollama LLM å®ä¾‹
        ollama_llm = OllamaLLM(config)
        print("âœ… Ollama LLM å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç®€å•çš„æ¶ˆæ¯
        messages = [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±"}
        ]
        
        print("ğŸ“¤ å‘é€æµ‹è¯•æ¶ˆæ¯...")
        response = await ollama_llm.acompletion(messages)
        print("âœ… æ”¶åˆ°å“åº”")
        
        # æå–æ–‡æœ¬å†…å®¹
        if isinstance(response, dict):
            if "error" in response:
                print(f"âŒ API é”™è¯¯: {response['error']}")
                if "raw_data" in response:
                    print(f"åŸå§‹æ•°æ®: {response['raw_data']}")
            else:
                try:
                    content = ollama_llm.get_choice_text(response)
                    print(f"âœ… å“åº”å†…å®¹: {content[:100]}...")
                except Exception as e:
                    print(f"âŒ æå–å†…å®¹å¤±è´¥: {e}")
                    print(f"å“åº”ç»“æ„: {json.dumps(response, indent=2, ensure_ascii=False)}")
        else:
            print(f"âŒ æ„å¤–å“åº”ç±»å‹: {type(response)}")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_json_parsing():
    """æµ‹è¯• JSON è§£æä¿®å¤"""
    print("\n=== æµ‹è¯• JSON è§£æä¿®å¤ ===")
    
    from metagpt.provider.ollama_api import OllamaMessageBase
    from metagpt.provider.general_api_requestor import OpenAIResponse
    
    # æ¨¡æ‹Ÿæ­£å¸¸çš„ JSON å“åº”
    normal_json = '{"message": {"role": "assistant", "content": "ä½ å¥½ï¼"}}'
    normal_response = OpenAIResponse(normal_json.encode('utf-8'), {})
    
    # æ¨¡æ‹Ÿæœ‰é—®é¢˜çš„ JSON å“åº”
    bad_json = '{"message": {"role": "assistant", "content": "ä½ å¥½ï¼"'  # ç¼ºå°‘ç»“æŸæ‹¬å·
    bad_response = OpenAIResponse(bad_json.encode('utf-8'), {})
    
    # æ¨¡æ‹Ÿæµå¼å“åº”ç‰‡æ®µ
    stream_json = 'data: {"response": "ä½ å¥½", "done": false}\n'
    stream_response = OpenAIResponse(stream_json.encode('utf-8'), {})
    
    # åˆ›å»ºæµ‹è¯•å®ä¾‹
    test_instance = OllamaMessageBase("test-model")
    
    try:
        # æµ‹è¯•æ­£å¸¸ JSON
        result1 = test_instance.decode(normal_response)
        print(f"âœ… æ­£å¸¸ JSON è§£æ: {result1}")
        
        # æµ‹è¯•æœ‰é—®é¢˜çš„ JSON
        result2 = test_instance.decode(bad_response)
        print(f"âœ… é—®é¢˜ JSON å¤„ç†: {result2}")
        
        # æµ‹è¯•æµå¼å“åº”
        result3 = test_instance.decode(stream_response)
        print(f"âœ… æµå¼å“åº”å¤„ç†: {result3}")
        
    except Exception as e:
        print(f"âŒ JSON è§£ææµ‹è¯•å¤±è´¥: {e}")

if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯• Ollama API ä¿®å¤æ•ˆæœ...")
    
    # æµ‹è¯• JSON è§£æä¿®å¤
    test_json_parsing()
    
    # æµ‹è¯•å®Œæ•´çš„ API è°ƒç”¨
    asyncio.run(test_ollama_api())
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼") 