#!/usr/bin/env python3
"""
Ollama é…ç½®æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ slc.py ä¸­çš„ Ollama é…ç½®æ˜¯å¦æ­£ç¡®å·¥ä½œ
"""

import sys
import os
sys.path.insert(0, '../metagpt')

from metagpt.tools.libs.slc import ollama_config, call_ollama, test_ollama_connection

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("=== æµ‹è¯•é…ç½®åŠ è½½ ===")
    print(f"æ¨¡å‹: {ollama_config.model}")
    print(f"åŸºç¡€URL: {ollama_config.base_url}")
    print(f"è¶…æ—¶æ—¶é—´: {ollama_config.timeout}ç§’")
    print(f"æ¸©åº¦å‚æ•°: {ollama_config.temperature}")
    print(f"æœ€å¤§Tokenæ•°: {ollama_config.max_tokens}")
    print(f"æµå¼å“åº”: {ollama_config.stream}")
    print("é…ç½®åŠ è½½æµ‹è¯•å®Œæˆ\n")

def test_simple_api_call():
    """æµ‹è¯•ç®€å•APIè°ƒç”¨"""
    print("=== æµ‹è¯•ç®€å•APIè°ƒç”¨ ===")
    prompt = "è¯·ç”¨Pythonå†™ä¸€ä¸ªç®€å•çš„Hello Worldç¨‹åº"
    print(f"å‘é€è¯·æ±‚: {prompt}")
    
    response = call_ollama(prompt)
    print(f"å“åº”ç»“æœ: {response[:200]}...")  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
    print("ç®€å•APIè°ƒç”¨æµ‹è¯•å®Œæˆ\n")

def test_connection():
    """æµ‹è¯•è¿æ¥"""
    print("=== æµ‹è¯•Ollamaè¿æ¥ ===")
    if test_ollama_connection():
        print("âœ… Ollamaè¿æ¥æµ‹è¯•æˆåŠŸ")
    else:
        print("âŒ Ollamaè¿æ¥æµ‹è¯•å¤±è´¥")
    print("è¿æ¥æµ‹è¯•å®Œæˆ\n")

def test_different_models():
    """æµ‹è¯•ä¸åŒæ¨¡å‹"""
    print("=== æµ‹è¯•ä¸åŒæ¨¡å‹ ===")
    models = ["qwen2.5:7b", "llama3:8b", "mistral:7b"]
    
    for model in models:
        print(f"æµ‹è¯•æ¨¡å‹: {model}")
        try:
            response = call_ollama("Hello", model=model)
            if "é”™è¯¯" not in response:
                print(f"âœ… æ¨¡å‹ {model} å·¥ä½œæ­£å¸¸")
            else:
                print(f"âŒ æ¨¡å‹ {model} è°ƒç”¨å¤±è´¥: {response}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹ {model} å¼‚å¸¸: {e}")
    print("ä¸åŒæ¨¡å‹æµ‹è¯•å®Œæˆ\n")

def test_parameters():
    """æµ‹è¯•ä¸åŒå‚æ•°"""
    print("=== æµ‹è¯•ä¸åŒå‚æ•° ===")
    
    # æµ‹è¯•ä¸åŒæ¸©åº¦
    temperatures = [0.1, 0.5, 0.9]
    for temp in temperatures:
        print(f"æµ‹è¯•æ¸©åº¦: {temp}")
        response = call_ollama("å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°", temperature=temp)
        print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
    
    # æµ‹è¯•ä¸åŒmax_tokens
    max_tokens_list = [100, 500, 1000]
    for tokens in max_tokens_list:
        print(f"æµ‹è¯•max_tokens: {tokens}")
        response = call_ollama("è§£é‡Šä»€ä¹ˆæ˜¯Python", max_tokens=tokens)
        print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
    
    print("å‚æ•°æµ‹è¯•å®Œæˆ\n")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Ollamaé…ç½®æµ‹è¯•\n")
    
    try:
        test_config_loading()
        test_connection()
        test_simple_api_call()
        test_different_models()
        test_parameters()
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 