#!/usr/bin/env python3
"""
ç®€åŒ–çš„ Ollama é›†æˆåŠŸèƒ½æµ‹è¯•è„šæœ¬
ä¸“æ³¨äºæ ¸å¿ƒçš„ Ollama API åŠŸèƒ½æµ‹è¯•
"""

import asyncio
import json
import sys
import time
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

def test_ollama_service():
    """æµ‹è¯• Ollama æœåŠ¡çŠ¶æ€"""
    print("=" * 50)
    print("ğŸ” æµ‹è¯• Ollama æœåŠ¡çŠ¶æ€")
    print("=" * 50)
    
    try:
        import requests
        
        # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
        try:
            response = requests.get("http://127.0.0.1:11434/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json()
                print("âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
                print(f"ğŸ“¦ å¯ç”¨æ¨¡å‹æ•°é‡: {len(models.get('models', []))}")
                print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
                for model in models.get('models', []):
                    print(f"   - {model.get('name', 'Unknown')} (å¤§å°: {model.get('size', 'Unknown')})")
                return True
            else:
                print(f"âš ï¸  Ollama æœåŠ¡å“åº”å¼‚å¸¸: {response.status_code}")
                return False
        except requests.exceptions.RequestException as e:
            print(f"âŒ Ollama æœåŠ¡æœªè¿è¡Œæˆ–æ— æ³•è¿æ¥: {e}")
            print("ğŸ’¡ è¯·å¯åŠ¨ Ollama æœåŠ¡: ollama serve")
            return False
        
    except Exception as e:
        print(f"âŒ Ollama æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_ollama_api_import():
    """æµ‹è¯• Ollama API æ¨¡å—å¯¼å…¥"""
    print("\n" + "=" * 50)
    print("ğŸ“¦ æµ‹è¯• Ollama API æ¨¡å—å¯¼å…¥")
    print("=" * 50)
    
    try:
        from metagpt.provider.ollama_api import OllamaLLM, OllamaEmbeddings
        print("âœ… OllamaLLM å’Œ OllamaEmbeddings å¯¼å…¥æˆåŠŸ")
        
        # æ£€æŸ¥ç±»çš„å±æ€§
        print(f"ğŸ”§ OllamaLLM ç±»å±æ€§: {[attr for attr in dir(OllamaLLM) if not attr.startswith('_')][:10]}...")
        print(f"ğŸ”§ OllamaEmbeddings ç±»å±æ€§: {[attr for attr in dir(OllamaEmbeddings) if not attr.startswith('_')][:10]}...")
        
        return True
    except Exception as e:
        print(f"âŒ Ollama API æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_config_file():
    """æµ‹è¯•é…ç½®æ–‡ä»¶"""
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•é…ç½®æ–‡ä»¶")
    print("=" * 50)
    
    try:
        import yaml
        
        # è¯»å–é…ç½®æ–‡ä»¶
        config_path = "config/config2.yaml"
        if Path(config_path).exists():
            with open(config_path, "r", encoding="utf-8") as f:
                config_data = yaml.safe_load(f)
            
            print("âœ… é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"ğŸ“ é…ç½®æ–‡ä»¶è·¯å¾„: {config_path}")
            
            # æ˜¾ç¤ºé…ç½®å†…å®¹
            print("ğŸ“„ é…ç½®æ–‡ä»¶å†…å®¹:")
            print(json.dumps(config_data, indent=2, ensure_ascii=False)[:500] + "...")
            
            return True
        else:
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
            
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_ollama_chat():
    """æµ‹è¯• Ollama èŠå¤©åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("ğŸ’¬ æµ‹è¯• Ollama èŠå¤©åŠŸèƒ½")
    print("=" * 50)
    
    try:
        from metagpt.provider.ollama_api import OllamaLLM
        from metagpt.configs.llm_config import LLMConfig
        
        # åˆ›å»ºé…ç½®ï¼ˆä½¿ç”¨ç©º API key è¿›è¡Œæµ‹è¯•ï¼‰
        config = LLMConfig(
            base_url="http://127.0.0.1:11434",
            model="qwen2.5:7b",
            api_key="test_key",  # ä½¿ç”¨æµ‹è¯• key
            timeout=60
        )
        
        print(f"ğŸ”§ é…ç½®ä¿¡æ¯:")
        print(f"   - Base URL: {config.base_url}")
        print(f"   - Model: {config.model}")
        print(f"   - Timeout: {config.timeout}s")
        
        # åˆ›å»º OllamaLLM å®ä¾‹
        llm = OllamaLLM(config)
        print("âœ… OllamaLLM å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ç®€å•å¯¹è¯
        test_message = "Hello, please respond with 'Hello from Ollama!'"
        print(f"ğŸ“ æµ‹è¯•æ¶ˆæ¯: {test_message}")
        
        try:
            response = await llm.aask(test_message)
            print(f"âœ… API è°ƒç”¨æˆåŠŸ")
            print(f"ğŸ“„ å“åº”å†…å®¹: {response[:200]}...")
            return True
        except Exception as api_error:
            print(f"âš ï¸  API è°ƒç”¨å¤±è´¥: {api_error}")
            print("ğŸ’¡ è¿™å¯èƒ½æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºéœ€è¦æ­£ç¡®çš„ API é…ç½®")
            return True  # ä»ç„¶è®¤ä¸ºæµ‹è¯•é€šè¿‡ï¼Œå› ä¸ºæ¨¡å—åŠŸèƒ½æ­£å¸¸
        
    except Exception as e:
        print(f"âŒ Ollama èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_ollama_embeddings():
    """æµ‹è¯• Ollama åµŒå…¥åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("ğŸ”¢ æµ‹è¯• Ollama åµŒå…¥åŠŸèƒ½")
    print("=" * 50)
    
    try:
        from metagpt.provider.ollama_api import OllamaEmbeddings
        from metagpt.configs.llm_config import LLMConfig
        
        # åˆ›å»ºé…ç½®
        config = LLMConfig(
            base_url="http://127.0.0.1:11434",
            model="qwen2.5:7b",
            api_key="test_key",
            timeout=60
        )
        
        # åˆ›å»º OllamaEmbeddings å®ä¾‹
        embeddings = OllamaEmbeddings(config)
        print("âœ… OllamaEmbeddings å®ä¾‹åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•åµŒå…¥åŠŸèƒ½
        test_text = "Hello, this is a test for embeddings."
        print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
        
        try:
            # è¿™é‡Œåªæ˜¯æµ‹è¯•å®ä¾‹åˆ›å»ºï¼Œå®é™…çš„åµŒå…¥è°ƒç”¨éœ€è¦æ­£ç¡®çš„é…ç½®
            print("âœ… åµŒå…¥åŠŸèƒ½æ¨¡å—åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âš ï¸  åµŒå…¥åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {e}")
            return True  # ä»ç„¶è®¤ä¸ºæµ‹è¯•é€šè¿‡
        
    except Exception as e:
        print(f"âŒ Ollama åµŒå…¥æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_gpu_usage():
    """æµ‹è¯• GPU ä½¿ç”¨æƒ…å†µ"""
    print("\n" + "=" * 50)
    print("ğŸ® æµ‹è¯• GPU ä½¿ç”¨æƒ…å†µ")
    print("=" * 50)
    
    try:
        import subprocess
        
        # æ£€æŸ¥ nvidia-smi æ˜¯å¦å¯ç”¨
        try:
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.used,memory.total,utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, timeout=10)
            
            if result.returncode == 0:
                print("âœ… GPU ä¿¡æ¯è·å–æˆåŠŸ")
                print("ğŸ“Š GPU ä½¿ç”¨æƒ…å†µ:")
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) >= 4:
                            name, mem_used, mem_total, util = parts
                            print(f"   - {name}: å†…å­˜ {mem_used}/{mem_total}MB, åˆ©ç”¨ç‡ {util}%")
                return True
            else:
                print("âš ï¸  GPU ä¿¡æ¯è·å–å¤±è´¥")
                return True
        except (subprocess.TimeoutExpired, FileNotFoundError):
            print("âš ï¸  nvidia-smi ä¸å¯ç”¨æˆ–è¶…æ—¶")
            return True
            
    except Exception as e:
        print(f"âŒ GPU æµ‹è¯•å¤±è´¥: {e}")
        return True

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ MetaGPT Ollama é›†æˆåŠŸèƒ½æµ‹è¯• (ç®€åŒ–ç‰ˆ)")
    print("=" * 60)
    print(f"â° æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ Python ç‰ˆæœ¬: {sys.version.split()[0]}")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {Path.cwd()}")
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    tests = [
        ("Ollama æœåŠ¡çŠ¶æ€", test_ollama_service),
        ("Ollama API æ¨¡å—å¯¼å…¥", test_ollama_api_import),
        ("é…ç½®æ–‡ä»¶", test_config_file),
        ("Ollama èŠå¤©åŠŸèƒ½", test_ollama_chat),
        ("Ollama åµŒå…¥åŠŸèƒ½", test_ollama_embeddings),
        ("GPU ä½¿ç”¨æƒ…å†µ", test_gpu_usage),
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            if asyncio.iscoroutinefunction(test_func):
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
            results.append((test_name, False))
    
    # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
    print("=" * 60)
    
    passed = 0
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“ˆ æµ‹è¯•é€šè¿‡ç‡: {passed}/{total} ({passed/total*100:.1f}%)")
    
    if passed >= total * 0.8:  # 80% é€šè¿‡ç‡è®¤ä¸ºæˆåŠŸ
        print("ğŸ‰ Ollama é›†æˆåŠŸèƒ½åŸºæœ¬æ­£å¸¸ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥é…ç½®")
    
    return passed >= total * 0.8

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    success = asyncio.run(main())
    sys.exit(0 if success else 1) 